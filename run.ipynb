{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchio as tio\n",
    "from torchio import AFFINE, DATA\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import Model\n",
    "import nibabel as nib\n",
    "import eval_utils\n",
    "import SimpleITK as sitk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "#Set a path to save the final weights to\n",
    "#PATH = \"\"\n",
    "\n",
    "#Settings and Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using gpu\")\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "bs_train, bs_val, bs_test = 16, 8, 1\n",
    "epochs = 6\n",
    "lr = 0.0001\n",
    "\n",
    "PATH = '/Users/raphaelbenichou/Desktop/weights_ml/model_state_dict.pt' \n",
    "\n",
    "#Create lists for the paths of training images and training masks \n",
    "isles_data_dir = '/Users/raphaelbenichou/Downloads/Dataset001_ISLES22forUNET'\n",
    "\n",
    "dwi_path_1 = os.path.join(isles_data_dir,'imagesTr/ISLES_247_0001.nii.gz')\n",
    "dwi_path_2 = os.path.join(isles_data_dir,'imagesTr/ISLES_248_0001.nii.gz')\n",
    "dwi_path_3 = os.path.join(isles_data_dir,'imagesTr/ISLES_249_0001.nii.gz')\n",
    "dwi_path_4 = os.path.join(isles_data_dir,'imagesTr/ISLES_250_0001.nii.gz')\n",
    "\n",
    "mask_path_1 = os.path.join(isles_data_dir, 'labelsTr/ISLES_247.nii.gz')\n",
    "mask_path_2 = os.path.join(isles_data_dir, 'labelsTr/ISLES_248.nii.gz')\n",
    "mask_path_3 = os.path.join(isles_data_dir, 'labelsTr/ISLES_249.nii.gz')\n",
    "mask_path_4 = os.path.join(isles_data_dir, 'labelsTr/ISLES_250.nii.gz')\n",
    "train_files = [dwi_path_1,dwi_path_2,dwi_path_3,dwi_path_4]\n",
    "mask_files = [mask_path_1,mask_path_2,mask_path_3,mask_path_4]\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"filename\": train_files, 'mask' : mask_files})\n",
    "df_train, df_test = train_test_split(df,test_size = 0.25)\n",
    "df_train, df_val = train_test_split(df_train,test_size = 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nibabel_to_pil(array_proxy):\n",
    "    # Load the data from the nibabel array proxy\n",
    "        array_data = np.array(array_proxy)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Convert the numpy array to a PIL image\n",
    "        pil_image = Image.fromarray(array_data.astype(np.uint8))\n",
    "        \n",
    "        return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "\n",
    "        image = nibabel_to_pil(image)\n",
    "        mask = nibabel_to_pil(mask)\n",
    "\n",
    "        # Resize\n",
    "        resize = transforms.Resize(size=(256, 256))\n",
    "        image = resize(image)\n",
    "        mask = resize(mask)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = (nib.load(self.image_paths[index])).dataobj[...,80]\n",
    "        mask = (nib.load(self.target_paths[index])).dataobj[...,80]\n",
    "        x, y = self.transform(image, mask)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "train_dataset = MyDataset(df_train[\"filename\"].values.tolist(), df_train[\"mask\"].values.tolist())\n",
    "val_dataset = MyDataset(df_val['filename'].values.tolist(), df_val['mask'].values.tolist())\n",
    "test_dataset = MyDataset(df_test['filename'].values.tolist(), df_test[\"mask\"].values.tolist())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs_train, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bs_val, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs_test, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_loss(pred, target):\n",
    "    smooth = 100\n",
    "\n",
    "    predf = pred.view(-1)\n",
    "    targetf = target.view(-1)\n",
    "    intersection = (predf * targetf).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (predf.sum() + targetf.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model.UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function \n",
    "def train(model, epochs):\n",
    "    \n",
    "    #Keep track of average training and validation losses for each epoch\n",
    "    avg_train_losses = []\n",
    "    avg_val_losses = []\n",
    "    \n",
    "    #Trigger for earlystopping\n",
    "    earlystopping = False \n",
    "\n",
    "    #Training loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #Record the training and validation losses for each batch in this epoch\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        loop = tqdm(enumerate(train_dataloader), total = len(train_dataloader), leave = False)\n",
    "        for batch, (images, targets) in loop:\n",
    "\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred = model(images)\n",
    "            loss = dc_loss(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "                        \n",
    "            with torch.no_grad():     #Show some samples at the first batch of each epoch \n",
    "                if batch == 1:\n",
    "                    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "                    model.eval()\n",
    "\n",
    "                    (img, mask) = next(iter(test_dataloader))\n",
    "                    img = img.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    mask = mask[0]\n",
    "                    pred = model(img)\n",
    "\n",
    "                    plt.figure(figsize=(12,12))\n",
    "                    plt.subplot(1,3,1)\n",
    "                    plt.imshow(np.squeeze(img.cpu().numpy()).transpose(1,2,0))\n",
    "                    plt.title('Original Image')\n",
    "                    plt.subplot(1,3,2)\n",
    "                    plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2), alpha=0.5)\n",
    "                    plt.title('Original Mask')\n",
    "                    plt.subplot(1,3,3)\n",
    "                    plt.imshow(np.squeeze(pred.cpu()) > .5)\n",
    "                    plt.title('Prediction')\n",
    "                    plt.show()\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():     #Record and print average validation loss for each epoch \n",
    "            for val_batch, (val_images, val_targets) in enumerate(val_dataloader):\n",
    "                val_images = val_images.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "                val_pred = model(val_images.detach())\n",
    "\n",
    "                val_loss = dc_loss(val_pred, val_targets).item()\n",
    "\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            epoch_avg_train_loss = np.average(train_losses)\n",
    "            epoch_avg_val_loss = np.average(val_losses)\n",
    "            avg_train_losses.append(epoch_avg_train_loss)\n",
    "            avg_val_losses.append(epoch_avg_val_loss)\n",
    "\n",
    "            print_msg = (f'train_loss: {epoch_avg_train_loss:.5f} ' + f'valid_loss: {epoch_avg_val_loss:.5f}')\n",
    "\n",
    "            print(print_msg)\n",
    "        \n",
    "        if epoch > 5:     #Early stopping with a patience of 1 and a minimum of 5 epochs \n",
    "            if avg_val_losses[-1]>=avg_val_losses[-2]:\n",
    "                print(\"Early Stopping Triggered With Patience 1\")\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                earlystopping = True \n",
    "        if earlystopping:\n",
    "            break\n",
    "\n",
    "    return  model, avg_train_losses, avg_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a2011e5855408aab4996e0160dd9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99831 valid_loss: 0.99681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c03b4961704c36a2b78d5132de42bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99822 valid_loss: 0.99681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9d79833b142c3826e5370532b6fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99815 valid_loss: 0.99680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260696f956144c95ab630dd3f90ebf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99810 valid_loss: 0.99680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7bfb151ac449eca5c790beccfb6390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99805 valid_loss: 0.99680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66238cd15ff34651b553f6230e33bcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.99800 valid_loss: 0.99679\n"
     ]
    }
   ],
   "source": [
    "best_model, avg_train_losses, avg_val_losses = train(model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/raphaelbenichou/Desktop/weights_ml/model_state_dict.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9acf810e1a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/raphaelbenichou/Desktop/weights_ml/model_state_dict.pt'"
     ]
    }
   ],
   "source": [
    "test_model = Model.UNet().to(device)\n",
    "test_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1e480deb0240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "(img, mask) = next(iter(test_dataloader))\n",
    "img = img.to(device)\n",
    "mask = mask.to(device)\n",
    "mask = mask[0]\n",
    "pred = model(img)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.squeeze(img.cpu().numpy()).transpose(3,2,0))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow((mask.cpu().numpy()).transpose(1,2,0).squeeze(axis=2), alpha=0.5)\n",
    "plt.title('Original Mask')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.squeeze(pred.cpu()) > .5)\n",
    "plt.title('Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "712b825af672f5eaff1c925e121dc858b8b3bcae7e86b71c7c2b0209b8fa1bca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
