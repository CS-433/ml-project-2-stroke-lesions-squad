{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7221977,"sourceType":"datasetVersion","datasetId":4179919},{"sourceId":7223431,"sourceType":"datasetVersion","datasetId":4181233},{"sourceId":7231064,"sourceType":"datasetVersion","datasetId":4186873},{"sourceId":7231104,"sourceType":"datasetVersion","datasetId":4186904}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchio","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:04:51.609255Z","iopub.execute_input":"2023-12-21T00:04:51.610003Z","iopub.status.idle":"2023-12-21T00:05:05.039722Z","shell.execute_reply.started":"2023-12-21T00:04:51.609968Z","shell.execute_reply":"2023-12-21T00:05:05.038795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports\nimport os\nimport random\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport gzip\nimport os\nimport shutil\n\n\nfrom tqdm import tqdm\n\nLEARNING_RATE = 5E-4\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nTRAIN_BATCH_SIZE = 16\nVAL_BATCH_SIZE = 2\nTEST_BATCH_SIZE = 2\n\nNUM_EPOCHS = 300\nNUM_WORKERS = os.cpu_count()\nIMAGE_HEIGHT = 128\nIMAGE_WIDTH = 128\nIMAGE_DEPTH = 128\nPATCH_SIZE = (64,64,64)\nNUM_PATCHES = 4\nPIN_MEMORY = True\nLOAD_MODEL = False\nTRAIN_IMG_DIR = \"/kaggle/input/full-dataset/Dataset001_ISLES22forUNET_uc/imagesTr\"\nTRAIN_MASK_DIR = \"/kaggle/input/full-dataset/Dataset001_ISLES22forUNET_uc/labelsTr\"\nCHECKPOINT_DIR = \"/kaggle/working/checkpoint\"\nSAVED_IMAGES_DIR = \"/kaggle/working/saved_images\"","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:05.041640Z","iopub.execute_input":"2023-12-21T00:05:05.041941Z","iopub.status.idle":"2023-12-21T00:05:08.973751Z","shell.execute_reply.started":"2023-12-21T00:05:05.041914Z","shell.execute_reply":"2023-12-21T00:05:08.972741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, 3, 1, 1, bias=True),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm3d(out_channels),\n            nn.Dropout3d(p=0.1),\n            nn.Conv3d(out_channels, out_channels, 3, 1, 1, bias=True),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm3d(out_channels),\n            nn.Dropout3d(p=0.1),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512,1024],\n    ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(nn.ConvTranspose3d(feature*2, feature, kernel_size=2, stride=2,))\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, img):\n        \"\"\"\n        Forward pass of the UNet\n        Parameters\n        ----------\n        img : The input image of shape (BATCH_SIZE, 3, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n\n        Returns: The output of the UNet of shape (BATCH_SIZE, 1, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n        -------\n\n        \"\"\"\n        # Connection is the list of outputs from the downsampling path.\n        # We save it to keep local information. So where is the information\n        connections = []\n        #path down the UNet, finds important informations\n        for down in self.downs:\n            img = down(img)\n            connections.append(img)\n            img = self.pool(img)\n\n        #link from downsampling to upsampling\n        img = self.bottleneck(img)\n\n        #reverse the connections list to go up the UNet\n        connections = connections[::-1]\n        for idx in range(0, len(self.ups), 2):\n            #ConvTranspose3d is the upsampling layer\n            img = self.ups[idx](img)\n            #concatenates the output from the upsampling layer with the output from the downsampling layer\n            connection = connections[idx//2]\n            if(img.shape != connection.shape):\n                new_connection = torch.zeros((connection.shape[0], connection.shape[1], img.shape[2], img.shape[3], img.shape[4]))\n                for i in range(connection.shape[0]):\n                    new_connection[i] = tio.Resize(img.shape[2:])(connection[i].cpu())\n                connection = new_connection.cuda()\n            #concatenat the image slices with the connection, along the channel axis\n            img = torch.cat((img, connection), dim=1)\n            #DoubleConv is the downsampling layer\n            img = self.ups[idx+1](img)\n\n        x = self.final_conv(img)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:08.975035Z","iopub.execute_input":"2023-12-21T00:05:08.975613Z","iopub.status.idle":"2023-12-21T00:05:08.993623Z","shell.execute_reply.started":"2023-12-21T00:05:08.975575Z","shell.execute_reply":"2023-12-21T00:05:08.992722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport nibabel as nib\nfrom sklearn.utils import shuffle\nimport copy\n\ndef remove_missing(image_paths, labels_paths):\n    \"\"\"\n    Remove images and labels that are missing\n    Parameters\n    ----------\n    image_paths : The paths to the images in the dataset of shape (NUM_IMAGES, NUM_CHANNELS) of the form \"Dataset001_ISLES22forUNET/imagesTr/ISLES_x_y.nii.gz\"\n    labels_paths : The paths to the labels in the dataset of shape (NUM_IMAGES) of the form \"Dataset001_ISLES22forUNET/labelsTr/ISLES_x.nii.gz\"\n\n    Returns : The image and label paths with the missing images removed\n    -------\n\n    \"\"\"\n    missing_images = [203,204]\n    missing_labels = [203,204]\n\n\n    labels_ids = []\n    image_ids = []\n\n    for i in range(len(labels_paths)):\n        labels_ids.append(labels_paths[i].split('_')[-1][:3])\n    for i in range(len(image_paths)):\n        image_ids.append(image_paths[i][0].split('_')[-2])\n\n    for i in range(len(image_paths)):\n        id = image_paths[i][0].split('_')[-2]\n        if id not in labels_ids:\n            missing_labels.append(i)\n\n\n    for i in range(len(labels_paths)):\n        id = labels_paths[i].split('_')[-1][:3]\n        if id not in image_ids:\n            missing_images.append(i)\n\n    image_paths = np.delete(image_paths, missing_labels, axis=0)\n    labels_paths = np.delete(labels_paths, missing_images, axis=0)\n\n    return image_paths, labels_paths\n\n\nclass MRIImage(Dataset):\n    def __init__(self, image_paths, labels_paths, transform=None, split_ratios=[0.5, 0.2, 0.1], mode = None , patch_size = (64, 64, 64),num_patches = 2):\n        \"\"\"\n        Create a dataset from a dataframe of images and labels.\n        Parameters\n        ----------\n        image_paths : The paths to the images in the dataset of shape (NUM_IMAGES * NUM_CHANNELS)\n        labels_paths : The paths to the labels in the dataset of shape (NUM_IMAGES)\n        \"\"\"\n        super(MRIImage, self).__init__()\n        self.transform = transform\n        self.split_ratios = split_ratios\n        self.mode = mode\n\n        images_list = np.array([os.path.join(image_paths, x) for x in os.listdir(image_paths)])\n        labels_list = np.array([os.path.join(labels_paths, x) for x in os.listdir(labels_paths)])\n\n        self.labels_paths = np.sort(labels_list)\n        self.image_paths = np.sort(images_list).reshape(-1, 3)\n\n        self.image_paths, self.labels_paths = remove_missing(self.image_paths, self.labels_paths)\n\n        num_training_imgs = len(self.labels_paths)\n        train_val_test = [int(x * num_training_imgs) for x in split_ratios]\n\n        selected = np.arange(0, num_training_imgs)\n        selected = shuffle(selected)\n\n        self.train_image_path = self.image_paths[selected[:train_val_test[0]]] #might want to add .values\n        self.train_label_path = self.labels_paths[selected[:train_val_test[0]]]\n        self.val_image_path = self.image_paths[selected[train_val_test[0]:train_val_test[0] + train_val_test[1]]]\n        self.val_label_path = self.labels_paths[selected[train_val_test[0]:train_val_test[0] + train_val_test[1]]]\n        self.test_image_path = self.image_paths[selected[train_val_test[0] + train_val_test[1]:]]\n        self.test_label_path = self.labels_paths[selected[train_val_test[0] + train_val_test[1]:]]\n\n        self.patch_size = patch_size  # Each channel is 128x128x128, patches will be 64x64x64\n        self.num_patches = num_patches\n        \n        self.previous_image = None\n        self.previous_index = None\n\n    def set_mode(self, mode):\n        if mode != \"train\" and mode != \"val\" and mode != \"test\":\n            raise ValueError(\"mode must be either train, val or test\")\n        self.mode = mode\n\n    def __len__(self):\n        if self.mode == \"train\":\n            return len(self.train_label_path) * self.num_patches\n        elif self.mode == \"val\":\n            return len(self.val_label_path)\n        elif self.mode == \"test\":\n            return len(self.test_label_path) * self.num_patches\n\n    def __getitem__(self, index):\n        \"\"\"\n        Get an image and its label from the dataset using an index.\n        Parameters\n        ----------\n        index : The index of the image to get\n\n        Returns\n        -------\n        original_images: The original images of shape (3, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n        mask: The mask of shape (IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n        -------\n\n        \"\"\"\n\n        #select the correct mode\n        if self.mode == \"train\":\n            image_paths = self.train_image_path[index//self.num_patches]\n            label_path = self.train_label_path[index//self.num_patches]\n            transform = self.transform[0]\n        elif self.mode == \"val\":\n            image_paths = self.val_image_path[index]\n            label_path = self.val_label_path[index]\n            transform = self.transform[1]\n        elif self.mode == \"test\":\n            image_paths = self.test_image_path[index//self.num_patches]\n            label_path = self.test_label_path[index//self.num_patches]\n            transform = self.transform[2]\n        else:\n            raise ValueError(\"mode must be either train, val or test\")\n\n        dwi_path = image_paths[0]\n        adc_path = image_paths[1]\n        flair_path = image_paths[2]\n\n        if(self.previous_index != index//self.num_patches):\n            self.previous_index = index//self.num_patches\n            original_images = self.create_image(adc_path, dwi_path, flair_path, label_path)\n            self.previous_image = original_images\n            if transform is not None:\n                # original_images[0:3] = histogram(original_images[0:3])\n                original_images = transform(original_images)\n                original_images[0:3] = normalize(original_images[0:3])\n                \n        original_images = self.previous_image\n\n        mask = np.array([original_images[3] > 0.5]).astype(np.float32)\n\n        if self.mode == \"val\":\n            return original_images[0:3], mask[0]\n\n        input_data = torch.tensor(original_images[0:3])\n        mask = torch.tensor(mask)\n\n        patch_idx = index % self.num_patches\n        start_indices = [\n            patch_idx * (sz // self.num_patches) for sz in input_data[0].shape\n        ]\n\n        # Calculate the end indices for each dimension\n        end_indices = [\n            start_indices[dim] + self.patch_size[dim] for dim in range(len(start_indices))\n        ]\n\n        # Extract the patch\n        channel_patch = []\n        for i in range(len(original_images) - 1):\n            channel_patch.append(input_data[i][\n                                 start_indices[0]:end_indices[0],\n                                 start_indices[1]:end_indices[1],\n                                 start_indices[2]:end_indices[2],\n                                 ])\n\n        channel_patch = tio.Resize(self.patch_size)(torch.stack(channel_patch))\n\n        mask_patch = mask[0][\n                     start_indices[0]:end_indices[0],\n                     start_indices[1]:end_indices[1],\n                     start_indices[2]:end_indices[2],\n                     ]\n\n        mask_patch = tio.Resize(self.patch_size)(mask_patch.unsqueeze(0))[0]\n\n        return channel_patch, mask_patch\n\n    def create_image(self, adc_path, dwi_path, flair_path, label_path):\n        \"\"\"\"\"\"\n        dwi_image = nib.load(dwi_path).get_fdata()\n        original_images = np.zeros((4, dwi_image.shape[0], dwi_image.shape[1], dwi_image.shape[2]))\n        original_images[0] = dwi_image\n        original_images[1] = nib.load(adc_path).get_fdata()\n        original_images[2] = nib.load(flair_path).get_fdata()\n        original_images[3] = nib.load(label_path).get_fdata()\n        return original_images\n\ndef normalize(image):\n    \"\"\"\n    Normalize a 3D image\n    Parameters\n    ----------\n    image: an image of shape (BATCH_SIZE, 3, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n\n    Returns: the normalized image\n    -------\n    \"\"\"\n    eps = 1e-10\n    min_value = np.min(image)\n    max_value = np.max(image)\n    norm_0_1 = (image - min_value) / (max_value - min_value + eps)\n\n    return np.clip(2*norm_0_1 - 1, -1, 1)\n\ndef get_train_val_test_Dataloaders(train_transforms, val_transforms, test_transforms):\n    dataset = MRIImage(TRAIN_IMG_DIR, TRAIN_MASK_DIR, [train_transforms, val_transforms, test_transforms], patch_size=PATCH_SIZE, num_patches=NUM_PATCHES)\n\n    train_set, val_set, test_set = copy.deepcopy(dataset), copy.deepcopy(dataset), copy.deepcopy(dataset)\n    train_set.set_mode('train')\n    val_set.set_mode('val')\n    test_set.set_mode('test')\n\n    train_dataloader = DataLoader(dataset=train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=True)\n    val_dataloader = DataLoader(dataset=val_set, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=True)\n    test_dataloader = DataLoader(dataset=test_set, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=True)\n\n    return train_dataloader, val_dataloader, test_dataloader","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:08.995852Z","iopub.execute_input":"2023-12-21T00:05:08.996314Z","iopub.status.idle":"2023-12-21T00:05:09.676111Z","shell.execute_reply.started":"2023-12-21T00:05:08.996271Z","shell.execute_reply":"2023-12-21T00:05:09.675327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#losses\n\ndef dice_coefficient(predicted, target, epsilon=1e-6):\n    intersection = torch.sum(predicted * target)\n    union = torch.sum(predicted) + torch.sum(target)\n    dice_score = (2.0 * intersection + epsilon) / (union + epsilon)\n    return dice_score\n\nclass DiceBCELoss_2(nn.Module):\n    def __init__(self, device=DEVICE):\n        super(DiceBCELoss_2, self).__init__()\n        self.device = device\n\n    def forward(self, predicted, target):\n        # Ensure predicted and target tensors are of the same shape\n        if predicted.shape != target.shape:\n            predicted = predicted.squeeze(1)\n        \n\n        sig_predicted = nn.Sigmoid()(predicted)\n        # Calculate Dice Loss\n        dice_loss = 1 - dice_coefficient(sig_predicted, target)\n\n        # Calculate Binary Cross Entropy Loss\n        bce_loss = nn.BCEWithLogitsLoss()(predicted, target).to(self.device)\n\n        # Combine both losses\n        combined_loss = 0.25*dice_loss + 0.75*bce_loss\n\n        return combined_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:09.677153Z","iopub.execute_input":"2023-12-21T00:05:09.677434Z","iopub.status.idle":"2023-12-21T00:05:09.685276Z","shell.execute_reply.started":"2023-12-21T00:05:09.677410Z","shell.execute_reply":"2023-12-21T00:05:09.684270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#utils\n\nfrom torch.utils.data import DataLoader\nimport os\nfrom torchmetrics.classification import *\nimport torch.nn.functional as F\n\n\ndef save_checkpoint(state,checkpoint_dir, epoch):\n    \"\"\"Saves model and training parameters at '{checkpoint_dir}/last_checkpoint.pytorch'.\n\n    Args:\n        state (dict): contains model's state_dict, optimizer's state_dict, epoch\n            and best evaluation metric value so far\n        checkpoint_dir (string): directory where the checkpoint are to be saved\n    \"\"\"\n\n    if not os.path.exists(checkpoint_dir):\n        os.mkdir(checkpoint_dir)\n\n    last_file_path = os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pytorch')\n    torch.save(state, last_file_path)\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None,\n                    model_key='state_dict', optimizer_key='optimizer'):\n    \"\"\"Loads model and training parameters from a given checkpoint_path\n    If optimizer is provided, loads optimizer's state_dict of as well.\n\n    Args:\n        checkpoint_path (string): path to the checkpoint to be loaded\n        model (torch.nn.Module): model into which the parameters are to be copied\n        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n            which the parameters are to be copied\n\n    Returns:\n        state\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        raise IOError(f\"Checkpoint '{checkpoint_path}' does not exist\")\n\n    state = torch.load(checkpoint_path, map_location='cuda')\n    model.load_state_dict(state[model_key])\n\n    if optimizer is not None:\n        optimizer.load_state_dict(state[optimizer_key])\n\n    return state\n\n\ndef train_metrics(predictions, targets, device):\n    \"\"\"\n    Calculate the accuracy and f1 score of the model\n    Parameters\n    ----------\n    predictions: The predictions of the model of shape (BATCH_SIZE, 1, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n    targets: The ground truth of shape (BATCH_SIZE, 1, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n    Returns: The accuracy and f1 score\n    -------\n    \"\"\"\n    if predictions.shape != targets.shape:\n        predictions = predictions.squeeze(1)\n    \n    predictions = nn.Sigmoid()(predictions)\n    predictions = (predictions > 0.5).long()\n    targets = (targets > 0.5).long()\n    tp = torch.logical_and(predictions == 1, targets == 1).sum().item()\n    tn = torch.logical_and(predictions == 0, targets == 0).sum().item()\n    fp = torch.logical_and(predictions == 1, targets == 0).sum().item()\n    fn = torch.logical_and(predictions == 0, targets == 1).sum().item()\n    dice=dice_coefficient(predictions,targets).item()\n\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    f1 = 2 * tp / (2 * tp + fp + fn+ 1e-10)\n    return accuracy, f1, tp, tn, fp, fn, dice\n\n\ndef check_accuracy(loader, model, crop_patch_size, device=\"cuda\"):\n    \"\"\"\n    Check the accuracy and f1 score of the model on the loader\n    Parameters\n    ----------\n    loader the validation loader\n    model the model to use\n    crop_patch_size the size of the patch to crop used before feeding the model\n    device the device to use, defaults to \"cuda\"\n\n    Returns : The accuracy and f1 score\n    -------\n\n    \"\"\"\n    num_correct = 0\n    num_pixels = 0\n    model.eval()\n\n    tp, tn, fp, fn, f1, accuracy,dice = 0, 0, 0, 0, 0, 0,0\n    num_iter = 0\n    for x, y in loader:\n        y = y.to(device)\n        sx, sy, sz = crop_patch_size[0], crop_patch_size[1], crop_patch_size[2]\n        #run over each patch\n        for i in range(0, y.shape[1], sx):\n            for j in range(0, y.shape[2], sy):\n                for k in range(0, y.shape[3], sz):\n                    crop_x = x[:, :, i:i + sx, j:j + sy, k:k + sz]\n                    crop_y = y[:, i:i + sx, j:j + sy, k:k + sz]\n                    binary_y = (crop_y > 0.5).float()\n                    with torch.no_grad():\n                        preds = compute_prediction(crop_patch_size, crop_x, model)\n\n                    accuracy_t, f1_t, tp_t, tn_t, fp_t, fn_t,dice_t = train_metrics(preds, binary_y, device)\n                    tp += tp_t\n                    tn += tn_t\n                    fp += fp_t\n                    fn += fn_t\n                    f1 += f1_t\n                    accuracy += accuracy_t\n                    dice+= dice_t\n\n                    num_iter += 1\n\n\n    print(\n        f\"Got average Accuracy : {accuracy/num_iter:.2f}\"\n    )\n    print(f\"True Positive: {tp}, True Negative: {tn}, False Positive: {fp}, False Negative: {fn}\")\n    print(f\"Got average F1 score: {f1/num_iter:.4f}\")\n    model.train()\n\n    return accuracy/num_iter, f1/num_iter, tp, tn, fp, fn, dice/num_iter\n\n\ndef compute_prediction(crop_patch_size, x, model):\n    \"\"\"\n    Compute the prediction of the model on an image x\n    Parameters\n    ----------\n    crop_patch_size the size of the patch to crop\n    crop_x the crop to use of shape (BATCH_SIZE, 3, CROP_DEPTH, CROP_HEIGHT, CROP_WIDTH)\n    model the model to use\n\n    Returns\n    -------\n\n    \"\"\"\n    # resize the patch to the correct size. Computationaly expensive because it calls the cpu\n    original_shape = x.shape\n    crop_x = resize_tensor(x, crop_patch_size)\n    # get the prediction\n    preds = torch.sigmoid(model(crop_x.float()))\n    preds = (preds > 0.5).float()\n    preds = resize_tensor(preds, original_shape[2:])\n    return preds\n\ndef resize_tensor(tensor: torch.Tensor, new_size):\n    \"\"\"\n    Resize the tensor to the new size\n    Parameters\n    ----------\n    tensor : The tensor to resize of shape (BATCH_SIZE, 3, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n    new_size : The new size of shape (IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)\n\n    Returns : The resized tensor\n    -------\n\n    \"\"\"\n    if tensor.shape[2:] == new_size:\n        return tensor\n    if tensor.shape[1:] == new_size:\n        return tensor\n\n    return F.interpolate(tensor, size=new_size, mode='trilinear', align_corners=False).to(tensor.device)\n\ndef save_predictions_as_imgs(\n        loader, model, crop_patch_size, epoch, folder=\"saved_images/\", device=\"cuda\"\n):\n    \"\"\"\n    Save the predictions of the model on the loader in the folder. Saves one image per batch.\n    Parameters\n    ----------\n    loader : The loader to use, one iteration of the loader must return the image and the mask of shape (BATCH_SIZE, 3, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH) and (BATCH_SIZE, IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH) respectively\n    model : The model to use\n    crop_patch_size : The size of the patch to crop\n    epoch : The epoch to save the images\n    folder : The folder to save the images, defaults to \"saved_images/\"\n    device : The device to use, defaults to \"cuda\"\n    \"\"\"\n    model.eval()\n    batch_idx = 0\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n\n    subfolder = f\"{folder}/epoch_{epoch}\"\n    if not os.path.exists(subfolder):\n        os.mkdir(subfolder)\n\n    for x, y in loader:\n        x = x.to(device=device)\n        y = y.to(device=device)\n        true_image = y[0]\n        full_pred = torch.zeros(y.shape[1], y.shape[2], y.shape[3])\n        sx, sy, sz = crop_patch_size[0], crop_patch_size[1], crop_patch_size[2]\n        #run over each patch\n        for i in range(0, y.shape[1], sx):\n            for j in range(0, y.shape[2], sy):\n                for k in range(0, y.shape[3], sz):\n                    crop_x = x[:, :, i:i + sx, j:j + sy, k:k + sz]\n                    with torch.no_grad():\n                        preds = compute_prediction(crop_patch_size, crop_x, model)\n\n                    full_pred[i:i + sx, j:j + sy, k:k + sz] = preds[0, 0]\n\n        for slice in range(0, full_pred.shape[0], full_pred.shape[0] // 4):\n            save_image(batch_idx, full_pred, slice, subfolder)\n            save_image(batch_idx, true_image, slice, subfolder)\n        batch_idx += 1\n        if batch_idx > 4:\n            break\n    model.train()\n\n\ndef save_image(batch_idx, img, slice, subfolder):\n    pred_image = img[slice]\n    torchvision.utils.save_image(pred_image, f\"{subfolder}/pred_{batch_idx}_slice{slice}.png\")\n    \ndef log(metrics, index, epoch):\n    \"\"\"\n    Log the metrics in a folder, creates the folder if it does not exist\n    Parameters\n    ----------\n    metrics a dictionary containing the metrics, such as loss, f1, accuracy, tp, tn, fp, fn\n    index  the index to log, either \"train\" or \"val\"\n    -------\n    \"\"\"\n    folder = f\"logs/{index}\"\n    if not os.path.isdir(folder):\n        os.makedirs(folder)\n    metrics_list = []\n    for key in metrics[index].keys():\n        metrics_list.append(metrics[index][key])\n    metrics_tensor = torch.tensor(metrics_list)\n    torch.save(metrics_tensor, f\"{folder}/metrics_epoch{epoch}.zip\")","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:09.686546Z","iopub.execute_input":"2023-12-21T00:05:09.686800Z","iopub.status.idle":"2023-12-21T00:05:12.373086Z","shell.execute_reply.started":"2023-12-21T00:05:09.686777Z","shell.execute_reply":"2023-12-21T00:05:12.372246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#main\n\nimport os\nimport numpy as np\nimport torchio as tio\nimport torch\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\n\n\ndef train_fn_patched(loader, model, optimizer, loss_fn, scaler):\n    \"\"\"\n    Train the model for one epoch\n    Parameters\n    ----------\n    loader: A dataloader of the training set\n    model: The model to train\n    optimizer: The optimizer to use\n    loss_fn: The loss function to use\n    scaler: The scaler to use for mixed precision training\n    -------\n\n    \"\"\"\n    \n    \n    model.train()\n    loop = tqdm(loader)\n    avg_loss = 0.0\n    batch_accuracy, batch_f1, batch_tp, batch_tn, batch_fp, batch_fn ,batch_dice= 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,0.0\n    number_iter = 0\n    total_loss = 0.0\n    for data, targets in loop:\n        \n        data = data.to(device=DEVICE)\n        targets = targets.float().to(device=DEVICE)\n\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data.float())\n            predictions=predictions.to(device=DEVICE)\n            loss = loss_fn(predictions, targets).to(device=DEVICE)\n\n        if np.isnan(loss.item()):\n            print(\"Nan loss encountered\")\n            print(model(data))\n            exit(1)\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        number_iter += 1\n        total_loss += loss.item()\n        loop.set_postfix(loss=total_loss / (number_iter + 1))\n        accuracy, f1, tp, tn, fp, fn,dice = train_metrics(predictions, targets, DEVICE)\n        batch_accuracy += accuracy\n        batch_f1 += f1\n        batch_tp += tp\n        batch_tn += tn\n        batch_fp += fp\n        batch_fn += fn\n        batch_dice+=dice\n\n    return total_loss/number_iter, batch_accuracy/number_iter, batch_f1/number_iter, batch_tp, batch_tn, batch_fp, batch_fn, batch_dice/number_iter\n\n\ndef main(backup_rate = 100):\n    #transform of a 3D image.\n\n    train_transform = tio.Compose([\n        #tio.RandomAffine(p=0.3),\n        \n        tio.CropOrPad((IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)),\n        #tio.RandomAnisotropy(p=0.1),\n        #tio.Blur(std=0.5, p=0.25),\n        #tio.RandomMotion(degrees=15, translation=5, p=0.3),\n        #tio.RandomBiasField(p=0.2),\n        tio.RandomFlip(p=0.3),\n        #tio.RandomElasticDeformation(max_displacement=10, p=0.05),\n        tio.RandomSwap(p=0.3),\n        # Normalization occurs later\n    ])\n    val_transform = tio.Compose([\n        \n        tio.CropOrPad((IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH)),\n    ])\n\n    #model definition\n    model = UNET(in_channels=3, out_channels=1)\n    model = nn.DataParallel(model).cuda()\n    model.to(DEVICE)\n\n    loss_fn = DiceBCELoss_2(device=DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=LEARNING_RATE/NUM_EPOCHS)\n\n    #Creating Dataloaders\n    train_loader, val_loader, test_loader = get_train_val_test_Dataloaders(train_transform, val_transform, val_transform)\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    losses = np.zeros(NUM_EPOCHS)\n    metrics = {\"train\" : {\"f1\": [], \"accuracy\": [], \"tp\": [], \"tn\": [], \"fp\": [], \"fn\": [],\"dice\":[]},\n               \"val\": {\"f1\": [], \"accuracy\": [], \"tp\": [], \"tn\": [], \"fp\": [], \"fn\": [],\"dice\":[]}}\n    #Traing in batches, save every 10 epochs\n    for epoch in range(NUM_EPOCHS):\n        losses[epoch], accuracy, f1, tp, tn, fp, fn,dice = train_fn_patched(train_loader, model, optimizer, loss_fn, scaler)\n        #print(f\"train acc : {accuracy}\")\n        print(f\"train f1 : {f1}\")\n        print(f\"train dice : {dice}\")\n        metrics[\"train\"][\"f1\"].append(f1)\n        metrics[\"train\"][\"accuracy\"].append(accuracy)\n        metrics[\"train\"][\"tp\"].append(tp)\n        metrics[\"train\"][\"tn\"].append(tn)\n        metrics[\"train\"][\"fp\"].append(fp)\n        metrics[\"train\"][\"fn\"].append(fn)\n        metrics[\"train\"][\"dice\"].append(dice)\n        log(metrics, \"train\", epoch)\n        # print some examples to a folder\n        if(epoch%backup_rate == 0 and epoch!=0):\n            save_predictions_as_imgs(\n                val_loader, model, PATCH_SIZE, epoch, folder=SAVED_IMAGES_DIR, device=DEVICE)\n\n            accuracy, f1, tp, tn, fp, fn,dice = check_accuracy(val_loader, model, PATCH_SIZE, device=DEVICE)\n            \n            metrics[\"val\"][\"f1\"].append(f1)\n            metrics[\"val\"][\"accuracy\"].append(accuracy)\n            metrics[\"val\"][\"tp\"].append(tp)\n            metrics[\"val\"][\"tn\"].append(tn)\n            metrics[\"val\"][\"fp\"].append(fp)\n            metrics[\"val\"][\"fn\"].append(fn)\n            metrics[\"val\"][\"dice\"].append(dice)\n            log(metrics, \"val\", epoch)\n\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n            }\n            save_checkpoint(checkpoint, CHECKPOINT_DIR, epoch)\n            \n    save_predictions_as_imgs(\n        val_loader, model, PATCH_SIZE,\"final\", folder=SAVED_IMAGES_DIR, device=DEVICE\n    )\n    accuracy, f1, tp, tn, fp, fn,dice = check_accuracy(val_loader, model, PATCH_SIZE, device=DEVICE)\n    metrics[\"val\"][\"f1\"].append(f1)\n    metrics[\"val\"][\"accuracy\"].append(accuracy)\n    metrics[\"val\"][\"tp\"].append(tp)\n    metrics[\"val\"][\"tn\"].append(tn)\n    metrics[\"val\"][\"fp\"].append(fp)\n    metrics[\"val\"][\"fn\"].append(fn)\n    metrics[\"val\"][\"dice\"].append(dice)\n    log(metrics, \"val\", epoch)\n\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint,CHECKPOINT_DIR,NUM_EPOCHS)\n    return losses, metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:12.374165Z","iopub.execute_input":"2023-12-21T00:05:12.374602Z","iopub.status.idle":"2023-12-21T00:05:12.865443Z","shell.execute_reply.started":"2023-12-21T00:05:12.374577Z","shell.execute_reply":"2023-12-21T00:05:12.864544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss,metrics=main()\n\nimport json\n\nif not os.path.exists(\"/kaggle/working/results\"):\n    os.mkdir(\"/kaggle/working/results\")\n\nloss_file_path = os.path.join(\"/kaggle/working/results\", \"loss.npy\")\nmetrics_file_path = os.path.join(\"/kaggle/working/results\", \"metrics.json\")\n\nnp.save(loss_file_path,loss)\n    \nwith open(metrics_file_path, 'w') as f: \n    json.dump(metrics, f)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:05:12.866563Z","iopub.execute_input":"2023-12-21T00:05:12.866821Z","iopub.status.idle":"2023-12-21T00:16:59.020754Z","shell.execute_reply.started":"2023-12-21T00:05:12.866798Z","shell.execute_reply":"2023-12-21T00:16:59.017258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n#!zip -r file.zip /kaggle/working/checkpoint/checkpoint_epoch50.pytorch\n#from IPython.display import FileLink\n#FileLink(r'file.zip')\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:16:59.022057Z","iopub.status.idle":"2023-12-21T00:16:59.022609Z","shell.execute_reply.started":"2023-12-21T00:16:59.022329Z","shell.execute_reply":"2023-12-21T00:16:59.022354Z"},"trusted":true},"execution_count":null,"outputs":[]}]}